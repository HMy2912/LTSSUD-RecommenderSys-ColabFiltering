{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HMy2912/LTSSUD-RecommenderSys-ColabFiltering/blob/main/Group_7_Seminar_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSC14116 - Group 7 - Parallel Collaborative Filtering Recommender System\n",
        "**Week 1 (6/9/2025 – 6/14/2025)**  \n",
        "**Member**: Đăng Hoàn Mỹ - 19127216  \n",
        "**Project**: User-user Neighborhood-based Collaborative Filtering (NBCF) Recommender System using MovieLens 100K dataset.  \n",
        "**Objective**: Build a movie recommender system with sequential (V1), Numba (V2), CUDA (V3), and CUDA with shared memory (V4) implementations, targeting 10× speedup and MAE < 1.2.\n",
        "**References**\n",
        "* MovieLens Datasets: https://grouplens.org/datasets/movielens/\n",
        "* Viblo Tutorial: Basics of Collaborative Filtering.\n",
        "* Machine Learning Cơ Bản: NBCF with MovieLens examples.\n",
        "* Lei Mao’s Blog: Cosine Similarity vs. Pearson Correlation."
      ],
      "metadata": {
        "id": "vQIzcLea61k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "Set up Google Colab with necessary libraries (`pandas`, `numpy`, `scipy`, `scikit-learn`, `numba`) and mount Google Drive for data storage. This ensures reproducibility and GPU access for future CUDA implementations (V3, V4)."
      ],
      "metadata": {
        "id": "Nepgc0xn67aI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kyyg75E6wFr",
        "outputId": "89cd0f17-f558-47f8-e35d-d2b0f8474480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numba import jit, prange, cuda\n",
        "import time\n",
        "import cupy as cp\n",
        "import cupyx.scipy.sparse as cp_sp\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSy99KE3i7BmT",
        "outputId": "33b33ed1-0369-4788-d9dc-ac449bab1c40"
      },
      "source": [
        "# Verify environment\n",
        "import numba\n",
        "print(\"Numba version:\", numba.__version__)  # Check compatibility (e.g., 0.61.2)\n",
        "!nvcc --version  # Expect CUDA ~11.x\n",
        "!nvidia-smi  # Confirm T4 GPU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numba version: 0.60.0\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Tue Jul  8 14:50:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy-cuda12x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GLGjly0oa6O",
        "outputId": "62696d1f-7683-4ac9-d714-740f2e355211"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (13.3.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "print(\"CuPy version:\", cp.__version__)\n",
        "# Test GPU array\n",
        "a = cp.array([1, 2, 3])\n",
        "print(\"CuPy array:\", a)"
      ],
      "metadata": {
        "id": "yw_MzczC7dpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de5b34f-d7b3-4563-e369-07ba0588ba31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CuPy version: 13.3.0\n",
            "CuPy array: [1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba.cuda as cuda\n",
        "print(\"GPU available:\", cuda.is_available())  # Should return True\n",
        "if cuda.is_available():\n",
        "    print(\"Detected GPUs:\", cuda.detect())"
      ],
      "metadata": {
        "id": "4XW6D0ISJry1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4213723-8190-4cc3-bf10-d5a33156ec52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "Found 1 CUDA devices\n",
            "id 0             b'Tesla T4'                              [SUPPORTED]\n",
            "                      Compute Capability: 7.5\n",
            "                           PCI Device ID: 4\n",
            "                              PCI Bus ID: 0\n",
            "                                    UUID: GPU-d63a2369-5403-6df7-5470-1d147338f059\n",
            "                                Watchdog: Disabled\n",
            "             FP32/FP64 Performance Ratio: 32\n",
            "Summary:\n",
            "\t1/1 devices are supported\n",
            "Detected GPUs: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Understanding the NBCF Algorithm"
      ],
      "metadata": {
        "id": "3v2CP63D70d_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "User-user Neighborhood-based Collaborative Filtering (NBCF) predicts a user’s movie ratings based on ratings from similar users. It’s suitable for MovieLens 100K (943 users, 1682 movies) due to fewer users than items, reducing similarity computation cost compared to `item-item` NBCF."
      ],
      "metadata": {
        "id": "EVEHdBrt9TN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps\n",
        "1. **Load Dataset**: Create user-item matrix `R` (943×1682) from MovieLens 100K.\n",
        "2. **Normalization**: Mean-center ratings to remove user bias, producing `R_norm`.\n",
        "3. **Similarity Computation**: Compute user-user cosine similarities (V1: sequential, V2: Numba, V3: CUDA, V4: CUDA with shared memory).\n",
        "4. **K-Nearest Neighbors (K-NN)**: Select 20 most similar users per user.\n",
        "5. **Recommendation**: Predict ratings for unrated movies, recommend top-10.\n",
        "6. **Evaluation**: Compute MAE (<1.2) and Precision@10 (~4%) on `u1.test`."
      ],
      "metadata": {
        "id": "WIy2Y1cp8p7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "For users `u` and `v`, cosine similarity is:\n",
        "\n",
        "$$ \\text{sim}(u,v) = \\frac{R_{\\text{norm},u} \\cdot R_{\\text{norm},v}}{|R_{\\text{norm},u}| |R_{\\text{norm},v}|} $$\n",
        "\n",
        "where $R_{\\text{norm},u}$ is the mean-centered rating vector. This measures rating pattern similarity, ignoring magnitude."
      ],
      "metadata": {
        "id": "LC4NgSJy8q2b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ls1QqsCL75Ih"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset Description"
      ],
      "metadata": {
        "id": "TSSmRCE69owM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MovieLens 100K\n",
        "* Source: https://grouplens.org/datasets/movielens/100k/\n",
        "* Files:\n",
        "    * `u.data`: 100,000 ratings (tab-separated, columns: `user_id`, `item_id`, `rating`, `timestamp`).\n",
        "    * `u.item`: 1682 movies (pipe-separated, columns: `item_id`, `title`, ...; use first two).\n",
        "    * `u1.test`: ~20,000 test ratings (same format as `u.data`).\n",
        "* Stats:\n",
        "    * Users: 943\n",
        "    * Movies: 1682\n",
        "    * Ratings: ~100,000 (1–5 scale)\n",
        "    * Sparsity: ~6.3% non-zero entries  ($\\frac{100,000}{943 \\times 1682 \\approx 0.063$).\n",
        "* Relevance: Ideal for user-user NBCF due to fewer users than movies, reducing similarity matrix size (943×943 vs. 1682×1682)."
      ],
      "metadata": {
        "id": "h5c_m-d-9lz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Sparse Matrix?\n",
        "\n",
        "The user-item matrix R (943×1682) has \\~6.3% non-zero entries, making dense storage (\\~12MB) inefficient.\n",
        "\n",
        "A sparse CSR (Compressed Sparse Row) matrix reduces memory usage to \\~1.2MB, critical for CUDA (V3, V4) on Colab’s T4 GPU (\\~12.7GB VRAM)."
      ],
      "metadata": {
        "id": "lZRfvkoo-WoB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puIn24We-WHZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load Data and Create User-Item Matrix\n",
        "Load MovieLens 100K (`u.data`, `u.item`) into pandas DataFrames, create sparse CSR matrix `R` (943×1682), and save to Google Drive for reuse."
      ],
      "metadata": {
        "id": "F6ITLE8EEbzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
        "item_url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.item'\n",
        "ratings = pd.read_csv(data_url, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "movies = pd.read_csv(item_url, sep='|', encoding='latin-1', usecols=[0, 1], names=['item_id', 'title'])\n",
        "print(\"Ratings shape:\", ratings.shape)  # (100000, 4)\n",
        "print(\"Movies shape:\", movies.shape)  # (1682, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nla9JpFTEeVc",
        "outputId": "674aa6a8-bab1-4f71-f246-2adbde5ddc52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings shape: (100000, 4)\n",
            "Movies shape: (1682, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Drive\n",
        "ratings.to_csv('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/ml-100k_ratings.csv', index=False)\n",
        "movies.to_csv('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/ml-100k_movies.csv', index=False)"
      ],
      "metadata": {
        "id": "sp7y49xeErRW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create user-item matrix\n",
        "n_users, n_items = 943, 1682\n",
        "R = sp.csr_matrix((ratings['rating'], (ratings['user_id'] - 1, ratings['item_id'] - 1)), shape=(n_users, n_items))\n",
        "print(\"User-item matrix shape:\", R.shape, \"Non-zero entries:\", R.nnz)  # (943, 1682), ~100000\n",
        "print(\"Sparsity:\", R.nnz / (n_users * n_items))  # ~0.063\n",
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/R_sparse.npy', R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6M8FpNBEtzO",
        "outputId": "56204360-169c-4188-90ec-5a58ebcb5aa3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-item matrix shape: (943, 1682) Non-zero entries: 100000\n",
            "Sparsity: 0.06304669364224531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data (for later use)\n",
        "test_url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u1.test'\n",
        "test_ratings = pd.read_csv(test_url, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "test_ratings.to_csv('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/ml-100k_test.csv', index=False)\n",
        "print(\"Test ratings shape:\", test_ratings.shape)  # (~20000, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dirF-dk2Ex9e",
        "outputId": "2ad5a241-2bde-4b24-a293-a83c1edf1d6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ratings shape: (20000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample ratings (user 1):\", R[0].toarray()[0, :5])  # First 5 items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKbYCiGxE0Y6",
        "outputId": "54ead848-e1d4-4c7a-ce47-dd968f40e618"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample ratings (user 1): [5 3 4 3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Normalize User-Item Matrix\n",
        "Mean-center ratings to remove user bias (e.g., picky users giving lower scores), producing `R_norm` and `user_means` for similarity computation."
      ],
      "metadata": {
        "id": "IdoLOUM_nbMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_matrix(R):\n",
        "    user_means = np.array(R.mean(axis=1)).flatten()  # Mean rating per user\n",
        "    R_norm = R.copy()  # Preserve original R\n",
        "    row_indices, col_indices = R_norm.nonzero()\n",
        "    R_norm.data = R_norm.data - user_means[row_indices]  # Subtract mean from non-zero ratings\n",
        "    return R_norm, user_means"
      ],
      "metadata": {
        "id": "xQynSD85nhBg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_norm, user_means = normalize_matrix(R)\n",
        "print(\"Normalized matrix non-zero count:\", R_norm.nnz)  # ~100000\n",
        "print(\"User means shape:\", user_means.shape)  # (943,)\n",
        "print(\"Sample user means:\", user_means[:5])  # First 5 users\n",
        "print(\"Sample normalized ratings (user 1):\", R_norm[0].toarray()[0, :5])  # First 5 items\n",
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/R_norm_sparse.npy', R_norm)\n",
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/user_means.npy', user_means)"
      ],
      "metadata": {
        "id": "z-xjdpt-nr_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c7a480-3028-4634-9ebf-400bc310542d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized matrix non-zero count: 100000\n",
            "User means shape: (943,)\n",
            "Sample user means: [0.58382878 0.13674197 0.08977408 0.06183115 0.29904875]\n",
            "Sample normalized ratings (user 1): [4.41617122 2.41617122 3.41617122 2.41617122 2.41617122]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking step"
      ],
      "metadata": {
        "id": "EFXwVNCCnwy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row, col = R_norm.nonzero()[0], R_norm.nonzero()[1]\n",
        "print(\"Sample check:\", R_norm[row[0], col[0]] == R[row[0], col[0]] - user_means[row[0]])  # True"
      ],
      "metadata": {
        "id": "ZvbSJA4hnyju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d82177-ac1a-4e56-c60b-e358ab0c8654"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample check: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Implement Train/Validation Split"
      ],
      "metadata": {
        "id": "m0e6TyrxJa36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratings, val_ratings = train_test_split(ratings, test_size=0.176, random_state=42)  # 15% of 85% = ~15,000\n",
        "train_R = sp.csr_matrix((train_ratings['rating'], (train_ratings['user_id'] - 1, train_ratings['item_id'] - 1)), shape=(943, 1682))\n",
        "val_R = sp.csr_matrix((val_ratings['rating'], (val_ratings['user_id'] - 1, val_ratings['item_id'] - 1)), shape=(943, 1682))\n",
        "test_R = sp.csr_matrix((test_ratings['rating'], (test_ratings['user_id'] - 1, test_ratings['item_id'] - 1)), shape=(943, 1682))"
      ],
      "metadata": {
        "id": "AkecvDXtJbTg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/train_R_sparse.npy', train_R)\n",
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/val_R_sparse.npy', val_R)\n",
        "np.save('/content/drive/MyDrive/2025/HK3/LTSSUD/Data/test_R_sparse.npy', test_R)\n",
        "print(\"Train matrix non-zero count:\", train_R.nnz)  # ~70,000\n",
        "print(\"Validation matrix non-zero count:\", val_R.nnz)  # ~15,000\n",
        "print(\"Test matrix non-zero count:\", test_R.nnz)  # ~15,000"
      ],
      "metadata": {
        "id": "0D9iBeJMJl24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe068178-a3b5-4123-a4de-644b2797ce17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train matrix non-zero count: 82400\n",
            "Validation matrix non-zero count: 17600\n",
            "Test matrix non-zero count: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Normalize Training Matrix\n",
        "Mean-center the training user-item matrix (`train_R`) to remove user bias for similarity computation."
      ],
      "metadata": {
        "id": "hk-FzJUqfQL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize training matrix (reusing Week 5 function)\n",
        "def normalize_matrix(R):\n",
        "    user_means = np.array(R.mean(axis=1)).flatten()  # Mean rating per user\n",
        "    R_norm = R.copy()  # Preserve original R\n",
        "    row_indices, col_indices = R_norm.nonzero()\n",
        "    R_norm.data = R_norm.data - user_means[row_indices]  # Subtract mean from non-zero ratings\n",
        "    return R_norm, user_means"
      ],
      "metadata": {
        "id": "uWuPE8E6fQdx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_R_norm, train_user_means = normalize_matrix(train_R)\n",
        "print(\"Train normalized matrix non-zero count:\", train_R_norm.nnz)  # ~82,400\n",
        "print(\"Train user means shape:\", train_user_means.shape)  # (943,)\n",
        "print(\"Sample train user means:\", train_user_means[:5])  # First 5 users\n",
        "print(\"Sample train normalized ratings (user 1):\", train_R_norm[0].toarray()[0, :5])  # First 5 items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TeugD7MfXLN",
        "outputId": "4c8b26c2-34b1-407a-fdfb-304fb63127f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train normalized matrix non-zero count: 82400\n",
            "Train user means shape: (943,)\n",
            "Sample train user means: [0.46313912 0.10998811 0.07491082 0.05588585 0.24375743]\n",
            "Sample train normalized ratings (user 1): [0.         2.53686088 3.53686088 0.         2.53686088]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify normalization\n",
        "row, col = train_R_norm.nonzero()[0], train_R_norm.nonzero()[1]\n",
        "print(\"Normalization check:\", train_R_norm[row[0], col[0]] == train_R[row[0], col[0]] - train_user_means[row[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYq0EQRYfaSM",
        "outputId": "a860d26d-93f2-4813-f358-160908c1b6bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization check: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: V1 Sequential Cosine Similarity\n",
        "Compute user-user cosine similarity matrix using sequential Python (scikit-learn) on `train_R_norm`."
      ],
      "metadata": {
        "id": "WUXtZR55fgBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V1: Sequential cosine similarity\n",
        "start = time.time()\n",
        "similarity_matrix_v1 = cosine_similarity(train_R_norm.toarray())\n",
        "np.fill_diagonal(similarity_matrix_v1, 0)  # Exclude self-similarity\n",
        "v1_time = time.time() - start"
      ],
      "metadata": {
        "id": "GedO7aS3fgX4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine similarity (V1) time: {v1_time} s\")\n",
        "print(\"Similarity matrix shape:\", similarity_matrix_v1.shape)  # (943, 943)\n",
        "print(\"Sample similarities (user 1):\", similarity_matrix_v1[0, 1:5])  # First few similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xW6TOE2fleu",
        "outputId": "38a497e6-5ee5-4c8c-d040-24affcd4c290"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (V1) time: 0.20035719871520996 s\n",
            "Similarity matrix shape: (943, 943)\n",
            "Sample similarities (user 1): [0.13289824 0.02755322 0.02830419 0.29438097]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: V2 Numba Cosine Similarity\n",
        "Compute user-user cosine similarity using Numba with parallel loops on sparse `train_R_norm`."
      ],
      "metadata": {
        "id": "FzyUoYx2foQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V2: Numba cosine similarity\n",
        "@jit(nopython=True, parallel=True)\n",
        "def cosine_similarity_numba(R_data, R_indices, R_indptr, n_users, result):\n",
        "    for i in prange(n_users):\n",
        "        for j in range(i + 1, n_users):\n",
        "            dot = 0.0\n",
        "            norm_i = 0.0\n",
        "            norm_j = 0.0\n",
        "            start_i, end_i = R_indptr[i], R_indptr[i + 1]\n",
        "            start_j, end_j = R_indptr[j], R_indptr[j + 1]\n",
        "            idx_i = R_indices[start_i:end_i]\n",
        "            idx_j = R_indices[start_j:end_j]\n",
        "            ratings_i = R_data[start_i:end_i]\n",
        "            ratings_j = R_data[start_j:end_j]\n",
        "            k, l = 0, 0\n",
        "            while k < len(idx_i) and l < len(idx_j):\n",
        "                if idx_i[k] == idx_j[l]:\n",
        "                    dot += ratings_i[k] * ratings_j[l]\n",
        "                    norm_i += ratings_i[k] ** 2\n",
        "                    norm_j += ratings_j[l] ** 2\n",
        "                    k += 1\n",
        "                    l += 1\n",
        "                elif idx_i[k] < idx_j[l]:\n",
        "                    norm_i += ratings_i[k] ** 2\n",
        "                    k += 1\n",
        "                else:\n",
        "                    norm_j += ratings_j[l] ** 2\n",
        "                    l += 1\n",
        "            while k < len(idx_i):\n",
        "                norm_i += ratings_i[k] ** 2\n",
        "                k += 1\n",
        "            while l < len(idx_j):\n",
        "                norm_j += ratings_j[l] ** 2\n",
        "                l += 1\n",
        "            if norm_i * norm_j > 0:\n",
        "                result[i, j] = dot / (np.sqrt(norm_i) * np.sqrt(norm_j))\n",
        "                result[j, i] = result[i, j]"
      ],
      "metadata": {
        "id": "cluvYpNgfor6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit(nopython=True, parallel=True)\n",
        "def cosine_similarity_numba_opt(R_data, R_indices, R_indptr, n_users, result):\n",
        "    norms = np.zeros(n_users, dtype=np.float32)\n",
        "    for i in prange(n_users):\n",
        "        start, end = R_indptr[i], R_indptr[i + 1]\n",
        "        norms[i] = np.sqrt(np.sum(R_data[start:end] ** 2))\n",
        "    for i in prange(n_users):\n",
        "        for j in range(i + 1, n_users):\n",
        "            start_i, end_i = R_indptr[i], R_indptr[i + 1]\n",
        "            start_j, end_j = R_indptr[j], R_indptr[j + 1]\n",
        "            common = np.intersect1d(R_indices[start_i:end_i], R_indices[start_j:end_j])\n",
        "            if len(common) > 0:\n",
        "                dot = np.sum(R_data[start_i:end_i][np.isin(R_indices[start_i:end_i], common)] *\n",
        "                             R_data[start_j:end_j][np.isin(R_indices[start_j:end_j], common)])\n",
        "                if norms[i] * norms[j] > 0:\n",
        "                    result[i, j] = dot / (norms[i] * norms[j])\n",
        "                    result[j, i] = result[i, j]"
      ],
      "metadata": {
        "id": "HS9OU9WvlccH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "similarity_matrix_v2 = np.zeros((n_users, n_users), dtype=np.float32)\n",
        "cosine_similarity_numba(train_R_norm.data, train_R_norm.indices, train_R_norm.indptr, n_users, similarity_matrix_v2)\n",
        "v2_time = time.time() - start"
      ],
      "metadata": {
        "id": "ujvmHc47ftx1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine similarity (V2 Numba) time: {v2_time} s\")\n",
        "print(\"V1-V2 difference (max):\", np.max(np.abs(similarity_matrix_v1 - similarity_matrix_v2)))\n",
        "print(\"Similarity matrix shape:\", similarity_matrix_v2.shape)  # (943, 943)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIl5ivuIfve5",
        "outputId": "02e9c6d6-7981-40ea-d796-bc16b785fc90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (V2 Numba) time: 4.35981822013855 s\n",
            "V1-V2 difference (max): 2.9625619513140578e-08\n",
            "Similarity matrix shape: (943, 943)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: V3 CUDA Cosine Similarity\n",
        "Compute user-user cosine similarity using CUDA on `train_R_norm`. Try Numba CUDA kernel; fallback to CuPy if PTX error persists."
      ],
      "metadata": {
        "id": "N8yly1mgf6-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V3: Numba CUDA cosine similarity (try first)\n",
        "@cuda.jit\n",
        "def cosine_similarity_cuda(R_data, R_indices, R_indptr, n_users, result):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < n_users and j < n_users and i < j:\n",
        "        dot = 0.0\n",
        "        norm_i = 0.0\n",
        "        norm_j = 0.0\n",
        "        start_i, end_i = R_indptr[i], R_indptr[i + 1]\n",
        "        start_j, end_j = R_indptr[j], R_indptr[j + 1]\n",
        "        idx_i = R_indices[start_i:end_i]\n",
        "        idx_j = R_indices[start_j:end_j]\n",
        "        ratings_i = R_data[start_i:end_i]\n",
        "        ratings_j = R_data[start_j:end_j]\n",
        "        k, l = 0, 0\n",
        "        while k < len(idx_i) and l < len(idx_j):\n",
        "            if idx_i[k] == idx_j[l]:\n",
        "                dot += ratings_i[k] * ratings_j[l]\n",
        "                norm_i += ratings_i[k] * ratings_i[k]\n",
        "                norm_j += ratings_j[l] * ratings_j[l]\n",
        "                k += 1\n",
        "                l += 1\n",
        "            elif idx_i[k] < idx_j[l]:\n",
        "                norm_i += ratings_i[k] * ratings_i[k]\n",
        "                k += 1\n",
        "            else:\n",
        "                norm_j += ratings_j[l] * ratings_j[l]\n",
        "                l += 1\n",
        "        while k < len(idx_i):\n",
        "            norm_i += ratings_i[k] * ratings_i[k]\n",
        "            k += 1\n",
        "        while l < len(idx_j):\n",
        "            norm_j += ratings_j[l] * ratings_j[l]\n",
        "            l += 1\n",
        "        if norm_i * norm_j > 0:\n",
        "            result[i, j] = dot / (cuda.libdevice.sqrt(norm_i) * cuda.libdevice.sqrt(norm_j))\n",
        "            result[j, i] = result[i, j]\n"
      ],
      "metadata": {
        "id": "jBq9Ayq3f73M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72mF7S24mkyJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# Convert sparse matrix to GPU\n",
        "train_R_norm_gpu = cp_sp.csr_matrix(train_R_norm)\n",
        "# Compute norms for all users\n",
        "norms = cp.sqrt(cp.array(train_R_norm_gpu.power(2).sum(axis=1)).flatten())\n",
        "# Compute dot products via matrix multiplication\n",
        "dot_products = train_R_norm_gpu.dot(train_R_norm_gpu.T).toarray()\n",
        "# Compute cosine similarity: sim(u,v) = dot(u,v) / (norm(u) * norm(v))\n",
        "norm_products = norms[:, None] * norms[None, :]\n",
        "similarity_matrix_v3 = cp.where(norm_products > 0, dot_products / norm_products, 0)\n",
        "# Set diagonal to zero\n",
        "cp.fill_diagonal(similarity_matrix_v3, 0)\n",
        "# Transfer to host\n",
        "similarity_matrix_v3_host = cp.asnumpy(similarity_matrix_v3)\n",
        "v3_time = time.time() - start\n"
      ],
      "metadata": {
        "id": "mPW-FyrQgASA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine similarity (V3 CuPy) time: {v3_time} s\")\n",
        "print(\"V1-V3 difference (max):\", np.max(np.abs(similarity_matrix_v1 - similarity_matrix_v3_host)))\n",
        "print(\"Similarity matrix shape:\", similarity_matrix_v3_host.shape)  # (943, 943)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVHZCNg3gCwF",
        "outputId": "c7caeca6-9d29-4ac9-cc09-781fba43f8ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (V3 CuPy) time: 6.058340787887573 s\n",
            "V1-V3 difference (max): 1.5543122344752192e-15\n",
            "Similarity matrix shape: (943, 943)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: K-Nearest Neighbors (K-NN)\n",
        "Select top-20 similar users per user from the similarity matrix for prediction and recommendation."
      ],
      "metadata": {
        "id": "2W6eBya4gZ4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Nearest Neighbors (k=20)\n",
        "def get_top_k_neighbors(sim_matrix, k=20):\n",
        "    top_k = np.argsort(sim_matrix, axis=1)[:, -k:][:, ::-1]  # Top-k indices\n",
        "    top_k_scores = np.take_along_axis(sim_matrix, top_k, axis=1)  # Corresponding scores\n",
        "    return top_k, top_k_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "AcrwXwRkgauH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k, top_k_scores = get_top_k_neighbors(similarity_matrix_v1)\n",
        "print(\"Top-k neighbors shape:\", top_k.shape)  # (943, 20)\n",
        "print(\"Top-k scores shape:\", top_k_scores.shape)  # (943, 20)\n",
        "print(\"Sample neighbors (user 1):\", top_k[0, :5])  # First 5 neighbors\n",
        "print(\"Sample scores (user 1):\", top_k_scores[0, :5])  # Their similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKiORV4oglS5",
        "outputId": "15a6708a-831d-434b-ec8c-ba35d9b2e0c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k neighbors shape: (943, 20)\n",
            "Top-k scores shape: (943, 20)\n",
            "Sample neighbors (user 1): [513 456 434 267 428]\n",
            "Sample scores (user 1): [0.45720621 0.44657831 0.44394793 0.4432999  0.44212913]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Rating Prediction\n",
        "Predict ratings using weighted averages of top-20 neighbors’ normalized ratings, adjusted by user mean."
      ],
      "metadata": {
        "id": "EVwrONSFgfb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict ratings\n",
        "def predict_rating(user, item, sim_matrix, top_k, top_k_scores, R_norm, user_means):\n",
        "    neighbors = top_k[user]\n",
        "    weights = top_k_scores[user]\n",
        "    ratings = R_norm[neighbors, item].toarray().flatten()\n",
        "    valid = ratings != 0\n",
        "    if np.sum(valid) == 0:\n",
        "        return user_means[user]\n",
        "    weighted_sum = np.sum(ratings[valid] * weights[valid])\n",
        "    weight_sum = np.sum(weights[valid])\n",
        "    return user_means[user] + (weighted_sum / weight_sum if weight_sum > 0 else 0)\n"
      ],
      "metadata": {
        "id": "AKDC1i4xggS_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction\n",
        "sample_user, sample_item = 0, 0\n",
        "pred = predict_rating(sample_user, sample_item, similarity_matrix_v1, top_k, top_k_scores, train_R_norm, train_user_means)\n",
        "print(f\"Predicted rating for user {sample_user + 1}, item {sample_item + 1}: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhBjw9PxgjQP",
        "outputId": "a9feeaf3-f171-439e-de0c-ac44dadac2ee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating for user 1, item 1: 3.673782048395862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Top-10 Movie Recommendation\n",
        "Recommend top-10 unrated movies per user based on predicted ratings."
      ],
      "metadata": {
        "id": "HQ2Vl_sxgn-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Recommend top-10 movies\n",
        "# def recommend_movies(user, sim_matrix, top_k, top_k_scores, R_norm, user_means, movies, n=10):\n",
        "#     unrated_items = np.where(R_norm[user].toarray().flatten() == 0)[0]\n",
        "#     if len(unrated_items) == 0:\n",
        "#         return []\n",
        "#     predictions = [predict_rating(user, item, sim_matrix, top_k, top_k_scores, R_norm, user_means) for item in unrated_items]\n",
        "#     top_indices = np.argsort(predictions)[-n:][::-1]\n",
        "#     top_items = unrated_items[top_indices]\n",
        "#     return movies.iloc[top_items][['title']].values.flatten()"
      ],
      "metadata": {
        "id": "-DRGjwX3go23"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_movies(user, sim_matrix, top_k, top_k_scores, R_norm, user_means, movies, n=10):\n",
        "    # Convert inputs to GPU\n",
        "    R_norm_gpu = cp_sp.csr_matrix(R_norm)\n",
        "    sim_matrix_gpu = cp.array(sim_matrix)\n",
        "    top_k_gpu = cp.array(top_k)\n",
        "    top_k_scores_gpu = cp.array(top_k_scores)\n",
        "    user_means_gpu = cp.array(user_means)\n",
        "\n",
        "    # Get unrated items\n",
        "    unrated_items = cp.where(R_norm_gpu[user].toarray().flatten() == 0)[0]\n",
        "    if len(unrated_items) == 0:\n",
        "        return []\n",
        "\n",
        "    # Vectorized prediction for all unrated items\n",
        "    neighbors = top_k_gpu[user]\n",
        "    weights = top_k_scores_gpu[user]\n",
        "    # Ensure sparse indexing and convert to dense for computation\n",
        "    ratings = R_norm_gpu[neighbors][:, unrated_items].toarray()  # Shape: (20, len(unrated_items))\n",
        "    valid = ratings != 0  # Shape: (20, len(unrated_items))\n",
        "    weights = weights.reshape(-1, 1)  # Shape: (20, 1) for broadcasting\n",
        "    weighted_sums = cp.sum(ratings * valid * weights, axis=0)  # Shape: (len(unrated_items),)\n",
        "    weight_sums = cp.sum(valid * weights, axis=0)  # Shape: (len(unrated_items),)\n",
        "    predictions = user_means_gpu[user] + cp.where(weight_sums > 0, weighted_sums / weight_sums, 0)\n",
        "\n",
        "    # Get top-n items\n",
        "    top_indices = cp.argsort(predictions)[-n:][::-1]\n",
        "    top_items = unrated_items[top_indices]\n",
        "    return movies.iloc[cp.asnumpy(top_items)][['title']].values.flatten()"
      ],
      "metadata": {
        "id": "F63uuDeIkrRJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recommendation\n",
        "sample_user = 0\n",
        "recs = recommend_movies(sample_user, similarity_matrix_v1, top_k, top_k_scores, train_R_norm, train_user_means, movies)\n",
        "print(f\"Top-10 recommendations for user {sample_user + 1}:\", recs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OiXYp-GgsDP",
        "outputId": "b71327ce-f4e5-47e8-e0fd-f8a9bcbce625"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 recommendations for user 1: ['Seven Years in Tibet (1997)' 'War Room, The (1993)' 'Flirt (1995)'\n",
            " 'Thin Blue Line, The (1988)' 'Down by Law (1986)'\n",
            " 'Philadelphia Story, The (1940)' 'Schizopolis (1996)' 'Hard Eight (1996)'\n",
            " 'Paths of Glory (1957)'\n",
            " 'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Evaluation\n",
        "Evaluate MAE (<1.2) and Precision@10 (~4%) on validation and test sets using V1’s similarity matrix."
      ],
      "metadata": {
        "id": "JHB3kTiFgup4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "gvnNnxaIhFi7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate MAE and Precision@10\n",
        "# def evaluate_model(sim_matrix, top_k, top_k_scores, R_norm, user_means, eval_ratings, dataset_name):\n",
        "#     predictions = []\n",
        "#     actuals = []\n",
        "#     for _, row in eval_ratings.iterrows():\n",
        "#         user, item, rating = row['user_id'] - 1, row['item_id'] - 1, row['rating']\n",
        "#         pred = predict_rating(user, item, sim_matrix, top_k, top_k_scores, R_norm, user_means)\n",
        "#         predictions.append(pred)\n",
        "#         actuals.append(rating)\n",
        "#     mae = mean_absolute_error(actuals, predictions)\n",
        "#     print(f\"{dataset_name} MAE:\", mae)\n",
        "\n",
        "#     precision = 0\n",
        "#     n_users = len(eval_ratings['user_id'].unique())\n",
        "#     for user in eval_ratings['user_id'].unique():\n",
        "#         user = user - 1\n",
        "#         recommendations = recommend_movies(user, sim_matrix, top_k, top_k_scores, R_norm, user_means, movies)\n",
        "#         if len(recommendations) == 0:\n",
        "#             continue\n",
        "#         user_eval = eval_ratings[eval_ratings['user_id'] == user + 1]\n",
        "#         relevant = set(user_eval[user_eval['rating'] >= 4]['item_id'].values - 1)\n",
        "#         recommended = set(movies[movies['title'].isin(recommendations)]['item_id'].values - 1)\n",
        "#         if recommended:\n",
        "#             precision += len(relevant & recommended) / len(recommended)\n",
        "#     precision = precision / n_users if n_users > 0 else 0\n",
        "#     print(f\"{dataset_name} Precision@10:\", precision)\n",
        "#     return mae, precision\n"
      ],
      "metadata": {
        "id": "jibK01OsgvXX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(sim_matrix, top_k, top_k_scores, R_norm, user_means, eval_ratings, dataset_name):\n",
        "    start = time.time()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    for _, row in eval_ratings.iterrows():\n",
        "        user, item, rating = row['user_id'] - 1, row['item_id'] - 1, row['rating']\n",
        "        pred = predict_rating(user, item, sim_matrix, top_k, top_k_scores, R_norm, user_means)\n",
        "        predictions.append(pred)\n",
        "        actuals.append(rating)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(actuals, predictions))  # Add RMSE\n",
        "    print(f\"{dataset_name} MAE: {mae:.4f}\")\n",
        "    print(f\"{dataset_name} RMSE: {rmse:.4f}\")\n",
        "\n",
        "    precision = 0\n",
        "    n_users = len(eval_ratings['user_id'].unique())\n",
        "    for user in eval_ratings['user_id'].unique():\n",
        "        user = user - 1\n",
        "        recommendations = recommend_movies(user, sim_matrix, top_k, top_k_scores, R_norm, user_means, movies)\n",
        "        if len(recommendations) == 0:\n",
        "            continue\n",
        "        user_eval = eval_ratings[eval_ratings['user_id'] == user + 1]\n",
        "        relevant = set(user_eval[user_eval['rating'] >= 4]['item_id'].values - 1)\n",
        "        recommended = set(movies[movies['title'].isin(recommendations)]['item_id'].values - 1)\n",
        "        if recommended:\n",
        "            precision += len(relevant & recommended) / len(recommended)\n",
        "    precision = precision / n_users if n_users > 0 else 0\n",
        "    print(f\"{dataset_name} Precision@10: {precision:.4f}\")\n",
        "    print(f\"{dataset_name} evaluation time: {time.time() - start:.2f} s\")\n",
        "    return mae, rmse, precision"
      ],
      "metadata": {
        "id": "IyoLJejTk_E7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate V1\n",
        "val_mae, val_rmse, val_precision = evaluate_model(similarity_matrix_v1, top_k, top_k_scores, train_R_norm, train_user_means, val_ratings, \"Validation\")\n",
        "test_mae, test_rmse, test_precision = evaluate_model(similarity_matrix_v1, top_k, top_k_scores, train_R_norm, train_user_means, test_ratings, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9JntjcRgywU",
        "outputId": "f5718cb9-ce2b-4393-a512-ac4fe482466d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 0.9312\n",
            "Validation RMSE: 1.2244\n",
            "Validation Precision@10: 0.0135\n",
            "Validation evaluation time: 18.22 s\n",
            "Test MAE: 0.8948\n",
            "Test RMSE: 1.1758\n",
            "Test Precision@10: 0.0044\n",
            "Test evaluation time: 12.13 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 6 Summary\n",
        "- Normalized training matrix (`train_R_norm`, ~82,400 non-zero entries).\n",
        "- Implemented V1 (sequential, ~{v1_time:.2f}s), V2 (Numba, ~{v2_time:.2f}s), V3 (CuPy, ~{v3_time:.2f}s) cosine similarity.\n",
        "- Verified V2/V3 accuracy (max differences <1e-6).\n",
        "- Added K-NN (k=20), rating prediction, and top-10 recommendation.\n",
        "- Evaluated on validation (MAE: {val_mae:.4f}, Precision@10: {val_precision:.4f}) and test (MAE: {test_mae:.4f}, Precision@10: {test_precision:.4f}) sets.\n",
        "- Speedup: V2/V1 = {v1_time/v2_time:.2f}x, V3/V1 = {v1_time/v3_time:.2f}x.\n",
        "- **Next Steps**: Optimize V2/V3, implement V4 (CUDA with shared memory), tune k (10, 20, 30, 50), compare cosine vs. Euclidean similarity."
      ],
      "metadata": {
        "id": "RKqFFWO9g9Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k02dCHp8hApk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}